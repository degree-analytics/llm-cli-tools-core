# v0.2.0 â€“ Shared Telemetry, Cost Insights, and AI PR Reviews

> Rollout target: Spacewalker, Mimir, and any other CLI tools consuming `llm-cli-tools-core`
>
> Release tag: `v0.2.0`

## ðŸš€ What shipped

### 1. Local Telemetry Storage
- Each AI call is now written to `.llm-telemetry/<date>/telemetry.jsonl`
- `summary.json` keeps running totals (cost, tokens, per-agent, per-model)
- Optional prompt/response capture with `LLM_STORE_PROMPTS=true` / `LLM_STORE_RESPONSES=true`
- Storage is enabled by default; disable with `LLM_TELEMETRY_STORAGE_ENABLED=false`

### 2. Config Management
- `.env` knobs:
  - `LLM_TELEMETRY_DIR` â€“ override storage path per project
  - `LLM_PROJECT_NAME` â€“ tag records (defaults to current directory name)
  - `LLM_PUSHGATEWAY_URL` â€“ still supported for Prometheus metrics
  - `LLM_CACHE_DIR` â€“ where pricing data caches (default `~/.cache/llm-cli-tools-core`)

### 3. Pricing Cache & Cost Estimation
- Weekly refresh against litellm + OpenRouter pricing feeds
- Auto-estimates cost when responses donâ€™t report `cost_usd`
- Cache stored in `~/.cache/llm-cli-tools-core/pricing.json`

### 4. `llm-telemetry costs` CLI
- Human table by default, JSON with `--json`
- Filters: `--project`, `--agent`, `--model`, `--status`, `--days` (30-day default)
- Example:
  ```bash
  llm-telemetry costs --project spacewalker --agent doc-finder --days 7
  ```

### 5. Claude & Codex PR Automations
- `/claude` or `@claude` and `/codex` or `@codex` comments now work in this repo
- `CLAUDE_MAX_TURNS` org variable set to 100
- Shared `.claude` workspace (GT agents, Ground Truth command, lint/search hooks)
- Refer to docs (below) for usage, GT workflow, and command catalogue

## âœ… Action items for downstream projects

### Spacewalker & Mimir
1. Bump dependency: `uv pip install "llm-cli-tools-core @ git+https://github.com/degree-analytics/llm-cli-tools-core@v0.2.0"`
2. Confirm `.env` contains project-specific telemetry path if needed
3. Ensure new CLI works (optional): `llm-telemetry costs --json`
4. Expect `.llm-telemetry/` directory to appear during AI runs; verify contents
5. Replace local `ai_telemetry.py` imports (already stubbed) â€“ no additional code changes required

### Other Consumers
- Same install/verify steps as above
- Add repo secrets if you want Claude/Codex workflows enabled (see below)

## ðŸ¤– PR Review Automations
- Secrets already configured at org level (`ANTHROPIC_API_KEY`, `OPENAI_API_KEY`)
- Optional: set repo variables for Claude cost overrides (`CLAUDE_INPUT_COST_PER_MTOK`, etc.)
- Trigger commands:
  - `@claude` *(default: docs, correctness, overengineering, justfile)*
  - `@claude review docs correctness`
  - `@codex` *(GPT-5 medium effort)*
- Output lands as sticky PR comments; metrics section includes run link

## ðŸ“š Supporting Docs
- `docs/claude-components/deployment-gt-workflow.md` â€“ required GT branch flow
- `docs/development/claude-commands.md` â€“ slash commands catalogue (Ground Truth, etc.)
- `docs/workflows/claude-review-workflows.md` â€“ review modes & category details
- `README.md` â€“ updated usage, storage, CLI instructions

## ðŸ§ª Validation Checklist
- `just lint check`
- `just test`
- `just ci`
- Manual `llm-telemetry costs --json` on sample data
- Sample Claude/Codex comment on PR #1

## ðŸ“¡ Heads-up
- Telemetry files can grow; rotate or archive `.llm-telemetry/` as needed
- Pricing cache refreshes automatically (7-day interval) â€“ can force refresh by deleting cache file
- Future work (v0.2.1+): usage/performance analytics subcommands, storage retention policies

Questions? Drop them in the project channel or file an issue. ðŸŽ‰
